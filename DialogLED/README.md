# DialogLED
**Pre-trained Model for Long Dialogue Understanding and Generation based on Longformer-Encoder-Decoder (LED)**

## Environment
`pip install -r requirements.txt`

## Pre-trained Models

We release the base version and large version of DialogLED in HuggingFace.
- **DialogLED-base-16384**: The input length during pre-training is truncated to 16,384.
- **DialogLED-large-5120**: The input length during pre-training is truncated to 5,120.

## Data Format
Each line in the data is a json file containing two keys: `src` is the input long dialogue and `tgt` is the corresponding reference summary.

Please download the datasets and unzip them to the `./data` folder.

## Model Output
We provide the summaries generated by DialogLED in `/model_output`.

## Example: AMI with DialogLED-large-5120

### Fine-tuning

```bash
export TOKENIZERS_PARALLELISM=true
export OMP_NUM_THREADS=1

CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8 \
python -m torch.distributed.launch --nproc_per_node 8 run_summarization.py \
    --model_name_or_path MingZhong/DialogLED-large-5120 \
    --do_train \
    --do_eval \
    --do_predict \
    --train_file data/AMI/train.json \
    --validation_file data/AMI/val.json \
    --test_file data/AMI/test.json \
    --text_column src \
    --summary_column tgt \
    --output_dir Models/AMI_DialogLED_large \
    --label_smoothing_factor 0.1 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 2 \
    --gradient_accumulation_steps 4 \
    --max_source_length 5120 \
    --max_target_length 360 \
    --learning_rate 2e-5 \
    --warmup_steps 50 \
    --max_steps 300 \
    --save_strategy steps \
    --evaluation_strategy steps \
    --save_steps 10 \
    --eval_steps 10 \
    --fp16 \
    --predict_with_generate \
```

### Decoding and Evaluation

```bash
export TOKENIZERS_PARALLELISM=true
export OMP_NUM_THREADS=1

CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
python -m torch.distributed.launch --nproc_per_node 8 run_summarization.py \
    --model_name_or_path Models/AMI_DialogLED_large/checkpoint-200 \
    --do_predict \
    --train_file data/AMI/train.json \
    --validation_file data/AMI/val.json \
    --test_file data/AMI/test.json \
    --text_column src \
    --summary_column tgt \
    --output_dir Models/AMI_DialogLED_large \
    --label_smoothing_factor 0.1 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 2 \
    --gradient_accumulation_steps 4 \
    --max_source_length 5120 \
    --max_target_length 360 \ 
    --fp16 \ 
    --predict_with_generate \
```

- Please select the suitable checkpoints according to the result of validation.
- If you want to use other datasets, adjust `--max_target_length` and `config.max_length` in `run_summarization.py`.
- If you want use the base version of DialogLED, set `--model_name_or_path` to `MingZhong/DialogLED-base-16384` in the finetuning phase, and set `--max_source_length` to 16384. 
- Note that the evaluation here shows the results of ROUGE score in HuggingFace, while the results of standard [pyrouge](https://github.com/bheinzerling/pyrouge) are reported in our paper. There is a slight difference between these two.
